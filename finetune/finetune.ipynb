{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import ConcatDataset, Subset\n",
    "\n",
    "import numpy as np\n",
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import OCRDataset, AlignCollate\n",
    "from utils import CTCLabelConverter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "LABEL_MAX_LENGTH = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import easyocr\n",
    "def get_training_convertor(ref_converter:easyocr.utils.CTCLabelConverter):\n",
    "    if isinstance(ref_converter, CTCLabelConverter):\n",
    "        return ref_converter\n",
    "    character = ''.join(ref_converter.character[1:])\n",
    "    converter = CTCLabelConverter(character)\n",
    "    converter.separator_list = ref_converter.separator_list\n",
    "    converter.ignore_idx = ref_converter.ignore_idx\n",
    "    converter.dict_list = ref_converter.dict_list\n",
    "    converter.dict = ref_converter.dict\n",
    "    return converter\n",
    "\n",
    "# setup model, converter\n",
    "reader = easyocr.Reader([\"ch_tra\"])\n",
    "model = reader.recognizer\n",
    "ref_converter = reader.converter\n",
    "character = ''.join(ref_converter.character[1:])\n",
    "converter = get_training_convertor(ref_converter)\n",
    "assert isinstance(converter, CTCLabelConverter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_FeatureFxtraction = True\n",
    "freeze_SequenceModeling = False\n",
    "\n",
    "if freeze_FeatureFxtraction:\n",
    "    for param in model.module.FeatureExtraction.parameters():\n",
    "        param.requires_grad = False\n",
    "if freeze_SequenceModeling:\n",
    "    for param in model.module.SequenceModeling.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss\n",
    "criterion = torch.nn.CTCLoss(zero_infinity=True).to(DEVICE)\n",
    "# loss_avg = Averager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer \n",
    "lr = 1.\n",
    "rho = 0.95\n",
    "eps = 1e-8\n",
    "filtered_parameters = [p for p in filter(lambda p:p.requires_grad, model.parameters())]\n",
    "optimizer = optim.Adadelta(filtered_parameters, lr=lr, rho=rho, eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignore data whose label is longer than 34: \n",
      "    filename                                words\n",
      "64    44.jpg  (895261) Greenery {Wemyss-Islamist}\n",
      "402  454.jpg  Tuktamysheva (resin) Technologies !\n",
      "427  490.jpg  Fourteenth . Naiads injurious_Issue\n",
      "498  571.jpg  Equalization LIGURIA carbohydrate [\n",
      "781  833.jpg  Buys-Horwood misinterpreting Twitch\n",
      "Ignore data whose label is longer than 34: \n",
      "    filename                                words\n",
      "64    44.jpg  (895261) Greenery {Wemyss-Islamist}\n",
      "402  454.jpg  Tuktamysheva (resin) Technologies !\n",
      "427  490.jpg  Fourteenth . Naiads injurious_Issue\n",
      "498  571.jpg  Equalization LIGURIA carbohydrate [\n",
      "781  833.jpg  Buys-Horwood misinterpreting Twitch\n"
     ]
    }
   ],
   "source": [
    "# setup dataset\n",
    "character = ''.join(ref_converter.character[1:])\n",
    "# print(character)\n",
    "\n",
    "training_set_roots = [\"./all_data/en_train\"]\n",
    "ocrs = [OCRDataset(root=root, character=character, label_max_length=34) for root in training_set_roots]\n",
    "ocr = ConcatDataset(ocrs)\n",
    "aligncollate = AlignCollate(imgH=64, imgW=600, keep_ratio_with_pad=False, contrast_adjust=0)\n",
    "train_loader = torch.utils.data.DataLoader(ocr, batch_size=32, collate_fn = aligncollate, shuffle=True)\n",
    "\n",
    "# aligncollate1 = AlignCollate(imgH=64, imgW=600, keep_ratio_with_pad=False, contrast_adjust=0.5)\n",
    "# train_loader1= torch.utils.data.DataLoader(ocr, batch_size=32, collate_fn = aligncollate1, shuffle=True)\n",
    "\n",
    "validation_set_roots = [\"./all_data/en_val\"]\n",
    "ocrs = [OCRDataset(root=root, character=character, label_max_length=34) for root in validation_set_roots]\n",
    "ocr = ConcatDataset(ocrs)\n",
    "val_loader = torch.utils.data.DataLoader(ocr, batch_size=32, shuffle=True, num_workers=6, collate_fn = aligncollate, prefetch_factor=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_epoch(model:torch.nn.Module, criterion:torch.nn.CTCLoss, convertor:CTCLabelConverter, optimizer:torch.optim.Optimizer, training_set_loader:torch.utils.data.DataLoader):\n",
    "    losses = []\n",
    "    for image_tensors, labels in training_set_loader:\n",
    "        image = image_tensors.to(DEVICE)\n",
    "        text, length = convertor.encode(labels)\n",
    "        batch_size = image.size(0)\n",
    "\n",
    "        preds = model(image, text).log_softmax(2)\n",
    "        preds_size = torch.IntTensor([[preds.size(1)]*batch_size])\n",
    "        preds = preds.permute(1,0,2)\n",
    "\n",
    "        torch.backends.cudnn.enabled = False\n",
    "        cost = criterion(preds, text.to(DEVICE), preds_size.to(DEVICE), length.to(DEVICE))\n",
    "        torch.backends.cudnn.enabled = True\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        cost.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(cost.cpu().detach().numpy())\n",
    "    \n",
    "    return np.asarray(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model:torch.nn.Module, \n",
    "               criterion:torch.nn.CTCLoss, \n",
    "               converter:CTCLabelConverter, \n",
    "               validation_set_loader:torch.utils.data.DataLoader,\n",
    "               *,\n",
    "               DEVICE= torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "    n_correct = 0\n",
    "    length_of_data = 0\n",
    "    losses = []\n",
    "    norm_EDs = []\n",
    "    norm_ED = 0\n",
    "    confidence_score_list = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image_tensors, labels in validation_set_loader:\n",
    "            image = image_tensors.to(DEVICE)\n",
    "            text, length = converter.encode(labels)\n",
    "            batch_size = image.size(0)\n",
    "\n",
    "            preds = model(image, text)\n",
    "            preds_size = torch.IntTensor([preds.size(1)]*batch_size)\n",
    "\n",
    "            # torch.backends.cudnn.enabled = False\n",
    "            cost = criterion(preds.log_softmax(2).permute(1,0,2), text, preds_size, length)\n",
    "            # torch.backends.cudnn.enabled = True\n",
    "\n",
    "            # decoding phase\n",
    "            _, preds_index = preds.max(2)\n",
    "            preds_index = preds_index.view(-1)\n",
    "            # preds_index = preds_index.cpu()\n",
    "            # preds_size = preds_size.cpu()\n",
    "            # print(f\"{preds_index.data=}, {preds_size.data=}\")\n",
    "            # assert False\n",
    "            preds_str = converter.decode_greedy(preds_index.data, preds_size.data)\n",
    "\n",
    "            # compute accuracy & confidence score\n",
    "            preds_prob = F.softmax(preds, dim=2)\n",
    "            preds_max_prob, _ = preds_prob.max(dim=2)\n",
    "\n",
    "            for gt,pred,pred_max_prob in zip(labels, preds_str, preds_max_prob):\n",
    "                if pred == gt:\n",
    "                    n_correct+=1\n",
    "                \n",
    "                if len(gt) == 0 or len(pred) ==0:\n",
    "                    norm_ED += 0\n",
    "                elif len(gt) > len(pred):\n",
    "                    norm_ED += 1 - edit_distance(pred, gt) / len(gt)\n",
    "                else:\n",
    "                    norm_ED += 1 - edit_distance(pred, gt) / len(pred)\n",
    "\n",
    "                confidence_score = pred_max_prob.cumprod(dim=0)[-1]\n",
    "                confidence_score_list.append(confidence_score)\n",
    "\n",
    "            length_of_data+=batch_size\n",
    "            losses.append(cost.cpu().detach().numpy())\n",
    "    \n",
    "    model.train()\n",
    "    accuracy = n_correct / float(length_of_data) *100\n",
    "    norm_ED = norm_ED / float(length_of_data)\n",
    "\n",
    "    return {\"average CTCLoss\":np.asarray(losses).mean(), \n",
    "            \"acc\":accuracy, \n",
    "            \"norm_ED\":norm_ED}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.5173155 0.26552072\n",
      "0 {'average CTCLoss': 1.098575, 'acc': 35.4586129753915, 'norm_ED': 0.7726093274870081}\n",
      "1 1.2287254 0.23853794\n",
      "1 {'average CTCLoss': 0.91733825, 'acc': 40.49217002237137, 'norm_ED': 0.7961566413441319}\n",
      "2 1.0110389 0.22275016\n",
      "2 {'average CTCLoss': 0.85392255, 'acc': 46.644295302013425, 'norm_ED': 0.8297752475766902}\n",
      "3 0.8667388 0.19530629\n",
      "3 {'average CTCLoss': 0.6273388, 'acc': 56.59955257270693, 'norm_ED': 0.8648087088654374}\n",
      "4 0.7435695 0.24744599\n",
      "4 {'average CTCLoss': 0.4804414, 'acc': 60.29082774049217, 'norm_ED': 0.8927824895091228}\n",
      "5 0.6320554 0.15996458\n",
      "5 {'average CTCLoss': 0.44590086, 'acc': 64.76510067114094, 'norm_ED': 0.9016212209157174}\n",
      "6 0.5525988 0.1609865\n",
      "6 {'average CTCLoss': 0.3680635, 'acc': 64.42953020134227, 'norm_ED': 0.9040629467131135}\n",
      "7 0.4951007 0.1480653\n",
      "7 {'average CTCLoss': 0.3740436, 'acc': 67.33780760626398, 'norm_ED': 0.9069915890523627}\n",
      "8 0.41774768 0.19233514\n",
      "8 {'average CTCLoss': 0.26936927, 'acc': 71.70022371364652, 'norm_ED': 0.9289680673727387}\n",
      "9 0.354183 0.11919561\n",
      "9 {'average CTCLoss': 0.26256418, 'acc': 74.83221476510067, 'norm_ED': 0.935448004840682}\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    result = training_epoch(model, criterion, converter, optimizer, training_set_loader=train_loader)\n",
    "    print(epoch, result.mean(), result.std())\n",
    "    val_result = validation(model, criterion, converter, val_loader)\n",
    "    print(epoch, val_result)\n",
    "    torch.save(model.state_dict(), f'./saved_models/OvO/iter_{epoch+1}.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
