{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import ConcatDataset, Subset\n",
    "\n",
    "import numpy as np\n",
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "cudnn.deterministic = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import OCRDatasetModified, AlignCollate\n",
    "from utils import CTCLabelConverter, Averager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "LABEL_MAX_LENGTH = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "\n",
    "import easyocr\n",
    "def get_training_convertor(ref_converter:easyocr.utils.CTCLabelConverter):\n",
    "    if isinstance(ref_converter, CTCLabelConverter):\n",
    "        return ref_converter\n",
    "    character = ''.join(ref_converter.character[1:])\n",
    "    converter = CTCLabelConverter(character)\n",
    "    converter.separator_list = ref_converter.separator_list\n",
    "    converter.ignore_idx = ref_converter.ignore_idx\n",
    "    converter.dict_list = ref_converter.dict_list\n",
    "    converter.dict = ref_converter.dict\n",
    "    return converter\n",
    "\n",
    "# setup model, converter\n",
    "reader = easyocr.Reader([\"ch_tra\"])\n",
    "model = reader.recognizer\n",
    "ref_converter = reader.converter\n",
    "character = ''.join(ref_converter.character[1:])\n",
    "converter = get_training_convertor(ref_converter)\n",
    "assert isinstance(converter, CTCLabelConverter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_FeatureFxtraction = True\n",
    "freeze_SequenceModeling = False\n",
    "\n",
    "if freeze_FeatureFxtraction:\n",
    "    for param in model.module.FeatureExtraction.parameters():\n",
    "        param.requires_grad = False\n",
    "if freeze_SequenceModeling:\n",
    "    for param in model.module.SequenceModeling.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss\n",
    "criterion = torch.nn.CTCLoss(zero_infinity=True).to(DEVICE)\n",
    "# loss_avg = Averager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer \n",
    "lr = 1.\n",
    "rho = 0.95\n",
    "eps = 1e-8\n",
    "filtered_parameters = [p for p in filter(lambda p:p.requires_grad, model.parameters())]\n",
    "optimizer = optim.Adadelta(filtered_parameters, lr=lr, rho=rho, eps=eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignore data whose label is longer than 34: \n",
      "    filename                                words\n",
      "64    44.jpg  (895261) Greenery {Wemyss-Islamist}\n",
      "402  454.jpg  Tuktamysheva (resin) Technologies !\n",
      "427  490.jpg  Fourteenth . Naiads injurious_Issue\n",
      "498  571.jpg  Equalization LIGURIA carbohydrate [\n",
      "781  833.jpg  Buys-Horwood misinterpreting Twitch\n",
      "Ignore data whose label is longer than 34: \n",
      "    filename                                words\n",
      "64    44.jpg  (895261) Greenery {Wemyss-Islamist}\n",
      "402  454.jpg  Tuktamysheva (resin) Technologies !\n",
      "427  490.jpg  Fourteenth . Naiads injurious_Issue\n",
      "498  571.jpg  Equalization LIGURIA carbohydrate [\n",
      "781  833.jpg  Buys-Horwood misinterpreting Twitch\n"
     ]
    }
   ],
   "source": [
    "# setup dataset\n",
    "character = ''.join(ref_converter.character[1:])\n",
    "# print(character)\n",
    "\n",
    "training_set_roots = [\"./all_data/en_train\"]\n",
    "ocrs = [OCRDatasetModified(root=root, character=character, label_max_length=34) for root in training_set_roots]\n",
    "ocr = ConcatDataset(ocrs)\n",
    "aligncollate = AlignCollate(imgH=64, imgW=600, keep_ratio_with_pad=False, contrast_adjust=0)\n",
    "train_loader = torch.utils.data.DataLoader(ocr, batch_size=32, collate_fn = aligncollate, shuffle=True)\n",
    "\n",
    "# aligncollate1 = AlignCollate(imgH=64, imgW=600, keep_ratio_with_pad=False, contrast_adjust=0.5)\n",
    "# train_loader1= torch.utils.data.DataLoader(ocr, batch_size=32, collate_fn = aligncollate1, shuffle=True)\n",
    "\n",
    "validation_set_roots = [\"./all_data/en_val\"]\n",
    "ocrs = [OCRDatasetModified(root=root, character=character, label_max_length=34) for root in validation_set_roots]\n",
    "ocr = ConcatDataset(ocrs)\n",
    "val_loader = torch.utils.data.DataLoader(ocr, batch_size=32, shuffle=True, num_workers=6, collate_fn = aligncollate, prefetch_factor=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_epoch(model:torch.nn.Module, criterion:torch.nn.CTCLoss, convertor:CTCLabelConverter, optimizer:torch.optim.Optimizer, training_set_loader:torch.utils.data.DataLoader):\n",
    "    losses = []\n",
    "    for image_tensors, labels in training_set_loader:\n",
    "        image = image_tensors.to(DEVICE)\n",
    "        text, length = convertor.encode(labels)\n",
    "        batch_size = image.size(0)\n",
    "\n",
    "        preds = model(image, text).log_softmax(2)\n",
    "        preds_size = torch.IntTensor([[preds.size(1)]*batch_size])\n",
    "        preds = preds.permute(1,0,2)\n",
    "\n",
    "        torch.backends.cudnn.enabled = False\n",
    "        cost = criterion(preds, text.to(DEVICE), preds_size.to(DEVICE), length.to(DEVICE))\n",
    "        torch.backends.cudnn.enabled = True\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        cost.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.)\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(cost.cpu().detach().numpy())\n",
    "    \n",
    "    return np.asarray(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model:torch.nn.Module, \n",
    "               criterion:torch.nn.CTCLoss, \n",
    "               converter:CTCLabelConverter, \n",
    "               validation_set_loader:torch.utils.data.DataLoader,\n",
    "               *,\n",
    "               DEVICE= torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n",
    "    n_correct = 0\n",
    "    length_of_data = 0\n",
    "    losses = []\n",
    "    norm_EDs = []\n",
    "    norm_ED = 0\n",
    "    confidence_score_list = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for image_tensors, labels in validation_set_loader:\n",
    "            image = image_tensors.to(DEVICE)\n",
    "            text, length = converter.encode(labels)\n",
    "            batch_size = image.size(0)\n",
    "\n",
    "            preds = model(image, text)\n",
    "            preds_size = torch.IntTensor([preds.size(1)]*batch_size)\n",
    "\n",
    "            # torch.backends.cudnn.enabled = False\n",
    "            cost = criterion(preds.log_softmax(2).permute(1,0,2), text, preds_size, length)\n",
    "            # torch.backends.cudnn.enabled = True\n",
    "\n",
    "            # decoding phase\n",
    "            _, preds_index = preds.max(2)\n",
    "            preds_index = preds_index.view(-1)\n",
    "            preds_index = preds_index.cpu()\n",
    "            preds_size = preds_size.cpu()\n",
    "            # print(f\"{preds_index.data=}, {preds_size.data=}\")\n",
    "            # assert False\n",
    "            preds_str = converter.decode_greedy(preds_index.data, preds_size.data)\n",
    "\n",
    "            # compute accuracy & confidence score\n",
    "            preds_prob = F.softmax(preds, dim=2)\n",
    "            preds_max_prob, _ = preds_prob.max(dim=2)\n",
    "\n",
    "            for gt,pred,pred_max_prob in zip(labels, preds_str, preds_max_prob):\n",
    "                if pred == gt:\n",
    "                    n_correct+=1\n",
    "                \n",
    "                if len(gt) == 0 or len(pred) ==0:\n",
    "                    norm_ED += 0\n",
    "                elif len(gt) > len(pred):\n",
    "                    norm_ED += 1 - edit_distance(pred, gt) / len(gt)\n",
    "                else:\n",
    "                    norm_ED += 1 - edit_distance(pred, gt) / len(pred)\n",
    "\n",
    "                confidence_score = pred_max_prob.cumprod(dim=0)[-1]\n",
    "                confidence_score_list.append(confidence_score)\n",
    "\n",
    "            length_of_data+=batch_size\n",
    "            losses.append(cost.cpu().detach().numpy())\n",
    "    \n",
    "    model.train()\n",
    "    accuracy = n_correct / float(length_of_data) *100\n",
    "    norm_ED = norm_ED / float(length_of_data)\n",
    "\n",
    "    return {\"average CTCLoss\":np.asarray(losses).mean(), \n",
    "            \"acc\":accuracy, \n",
    "            \"norm_ED\":norm_ED}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3729794 0.66403234\n",
      "(1.4258851, 28.859060402684566, 0.7128050372282175)\n",
      "1.47491 0.28819767\n",
      "(1.1324071, 32.88590604026846, 0.7485286340649494)\n",
      "1.1652516 0.29703745\n",
      "(0.9320845, 43.064876957494405, 0.8029135862399721)\n",
      "0.9866853 0.20075099\n",
      "(0.7221694, 47.98657718120805, 0.8430779400924748)\n",
      "0.8300511 0.20905429\n",
      "(0.59703606, 55.70469798657718, 0.8723255568649328)\n",
      "0.7169686 0.19908415\n",
      "(0.5108433, 61.07382550335571, 0.8929523013505967)\n",
      "0.63380766 0.18276386\n",
      "(0.40636724, 64.42953020134227, 0.9090372857039414)\n",
      "0.56100684 0.19664545\n",
      "(0.37920424, 64.5413870246085, 0.8972572521431676)\n",
      "0.45578715 0.107737504\n",
      "(0.3061997, 71.81208053691275, 0.9298603969579857)\n",
      "0.40657172 0.10174054\n",
      "(0.23246852, 74.16107382550335, 0.9391848116875982)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    result = training_epoch(model, criterion, converter, optimizer, training_set_loader=train_loader)\n",
    "    print(epoch, result.mean(), result.std())\n",
    "    val_result = validation(model, criterion, converter, val_loader)\n",
    "    print(epoch, val_result)\n",
    "    torch.save(model.state_dict(), f'./saved_models/OvO/iter_{epoch+1}.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
